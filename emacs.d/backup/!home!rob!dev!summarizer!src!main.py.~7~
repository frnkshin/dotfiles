# MAIN.PY
# Written by Hansol Shin on Nov-30-2017
from nltk.stem import SnowballStemmer
from gensim.summarization import summarize
from gensim.models import Word2Vec
from sklearn import svm

from Dataset import FakeNews, RealNews
from datetime import datetime

import numpy as np
import re
import time
import nltk
import progressbar
import inquirer


NUM_FAKES = 12999
NUM_REALS = 4515

# Normalizes words to base words
def normalize(text):
    text = " ".join(str(x) for x in nltk.sent_tokenize(text))
    new_text = []
    stemmer = SnowballStemmer("english")
    for sentence in text:
        sentence = sentence.lower()
        sentence = re.sub("((\S+)?(http(s)?)(\S+))|((\S+)?(www)(\S+))|((\S+)?(\@)(\S+)?)", " ", sentence)
        sentence = re.sub("[^a-z ]", "", sentence)
        sentence = nltk.word_tokenize(sentence)
        sentence = [word for word in sentence if len(word) > 1]  # exclude 1 letter words
        sentence = [stemmer.stem(word) for word in sentence]
        new_text.append(sentence)

    return new_text

def rogue(model, gold):
    score = 0
    gold = gold.split(' ')
    model = model.split(' ')

    score = len(set(gold) & set(model)) / len(gold)
    return score

def normalize_set(dataset, num_count):
    with progressbar.ProgressBar(max_value=num_count) as progress:
        i = 0
        for data in dataset:
            dataset[data]['head'] = normalize(dataset[data]['head'])
            dataset[data]['text'] = normalize(dataset[data]['text'])
            try:
                dataset[data]['summary'] = normalize(dataset[data]['summary'])
            except KeyError:
                pass
            time.sleep(0.001)
            i += 1
            progress.update(i)
        return dataset

def summarize_set(dataset, num_count):
    with progressbar.ProgressBar(max_value=NUM_FAKES) as progress:
        scores = 0
        i = 0
        output = open("../res/fake_summaries.txt", 'w+')
        output.write('\n')

        for data in dataset:
            try:
                dataset[data]['summary'] = summarize(normalize(dataset[data]['text']), word_count=50)
            except ValueError:
                dataset[data]['summary'] = normalize(dataset[data]['text'])
            output.write(dataset[data]['summary'])
            scores += rogue(dataset[data]['text'], dataset[data]['summary'])
            time.sleep(0.001)
            i += 1
            progress.update(i)
        print(str(scores / NUM_FAKES))

def load_summaries(dataset, num_count, path):
    with open(path, 'r') as f:
        for data in dataset:
            dataset[data]['summary'] = f.readline()
    return dataset

def load_normalized(dataset, num_count, path):
    with open(path, 'r') as f:
        for data in dataset:
            dataset[data]['head'] = f.readline()
            dataset[data]['text'] = f.readline()
            dataset[data]['summary'] = f.readline()
    return dataset

# Main Function
if __name__ == "__main__":
    start = datetime.now()

    # LOAD DATASETS
    fake = FakeNews()
    real = RealNews()
    fake_news = fake.load_data()
    real_news = real.load_data()

    # NORMALIZATION
    confirm = {
        inquirer.Confirm('confirmed',
                         message="Do you want to normalize data?",
                         default=True),
    }
    confirmation = inquirer.prompt(confirm)
    if confirmation['confirmed']:
        # Normalize real news text
        real_news = normalize_set(real_news, NUM_REALS)
        fake_news = normalize_set(fake_news, NUM_FAKES)
        real.save("../res/real_normalized.txt")
        fake.save("../res/fake_normalized.txt")
    else:
        real.data = load_normalized(real_news, NUM_REALS, "../res/real_normalized.txt")
        fake.data = load_normalized(fake_news, NUM_FAKES, "../res/fake_normalized.txt")

    # SUMMARIZATION
    confirm = {
        inquirer.Confirm('confirmed',
                         message="Do you want to summarize data?",
                         default=True),
    }
    confirmation = inquirer.prompt(confirm)
    if confirmation['confirmed']:
        fake_news = summarize_set(fake_news, NUM_FAKES)
    else:
        fake_news = load_summaries(fake_news, NUM_FAKES, "../res/fake_summaries.txt")

    # WORD2VEC modeling
    all_sentences = []
    for news in real_news:
        all_sentences.append(real_news[news]['summary'].split(' '))
    print(datetime.now() - start)

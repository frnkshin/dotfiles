% Created 2017-12-19 Tue 06:51
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{setspace}
\singlespacing
\usepackage[margin=1.0in]{geometry}
\usepackage{mdframed}
\BeforeBeginEnvironment{minted}{\begin{mdframed}}
\AfterEndEnvironment{minted}{\end{mdframed}}
\author{Hansol Shin}
\date{December 18, 2017}
\title{Using Word2Vec to Differentiate\\Fake and Real News}
\hypersetup{
 pdfauthor={Hansol Shin},
 pdftitle={Using Word2Vec to Differentiate\\Fake and Real News},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.3.1 (Org mode 9.1.3)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{center}
\textbf{\textbf{Abstract}}

The goal of this project is to successfully classify different documents as fake news or real news based on the results of pre-processing and various learning algorithms. This uses a neural network model, Word2Vec, to create two models, each representing fake and real news. Each model is trained with the dataset the model belongs to. After the training is done for each model, a set of test data is fed into the models to determine whether the test data more resembles fake news or the real news.
\end{center}

\newpage


\section{Introduction}
\label{sec:orgebee24b}
\begin{quote}
This project aims to use Word2Vec model along with various other learning algorithms in order to detect a class for a news, whether it is fake or real. This helps in the news readers to distinguish the credibility of their source in a better way. Word2Vec model transforms each sentences into numerical vector form by running computations in a neural network.
\end{quote}


\section{Problem Statement}
\label{sec:orgfe5bc13}
\begin{quote}
The main problem this project aims to solve is fake news detection. A famous depiction of the underlying problem that rising number of fake news creates is Pizza Gate Scandal. The Pizza Gate Scandal happened in the 2016 Presidential Election, where a fake news source published a fake news revealing a secret hideout of a child-trafficking gang led by Hillary Clinton, then democratic partyâ€™s runner of POTUS, to be a pizzaria based in the D.C. A man, known to be Trump supporter, actually believed in the news and brought his gun to rescue the poor children that were victimized by Hillary Clinton. The man was arrested, and this raised concern for the flooding population of fake news into the daily lives of the public.
\end{quote}

\subsection{Notations}
\label{sec:org9b1384a}

\begin{mdframed}
\begin{center}
\begin{tabular}{ll}
\textbf{\textbf{Symbol}} & \textbf{\textbf{Description}}\\
W2V & Word2Vec\\
Cmin & Minimum word occurrence threshold for W2V model\\
HS & Hierarchial Softmax\\
TP & True Positive (True Real)\\
TN & True Negative (True Fake)\\
FP & False Positive (False Real)\\
FN & False Negative (False Fake)\\
\end{tabular}
\end{center}
\end{mdframed}


\section{Literature Review}
\label{sec:orgebcb99d}

\subsection{Extractive Summarization using Deep Learning}
\label{sec:orgecf4bcc}
\begin{quote}
Initial approach to the project was to summarize news articles and use SVM to create a classifier to divide the two classes -- fake news and real news. However, text summarization yielded some problems by shrinking the meaning of the text in an abstract manner. One useful idea taken from this paper was about preprocessing approach to generalize a sentence and create a better representation which may be computationally faster to process. Number of techniques were dealt in the paper, however only few techniques were used, which include \emph{word normalization}, \emph{stop word filtering}, and \emph{parts-of-speech filtering}. The paper discussed about the use of \emph{Restricted Boltzmann Machine} to generate summaries for document, but \emph{Restricted Boltzmann Machine} was not used for summary generation.
\end{quote}

\subsection{Distributed Representations of Words and Phrases and their Compositionality}
\label{sec:orgb07ac4e}
\begin{quote}
This introduces Word2Vec, and discusses in depth about how the revolutionary modeling greatly helps in natural language processing. Mainly there are two different models -- Skip-gram and CBOW.
\end{quote}
\begin{quote}
Skip-gram is the original modeling technique used in this specific paper. Skip-gram weighs the relationship between words by calculating the global distance of the word pairs in global set of sentences to generate similarity between to words. CBOW, on the other hand, is a simpler modeling that is based on the bag-of-words model used by gensim library. Since a lot of the news data contained multiple number of sentences skip-gram model was preferred. However a lot of the news articles also consisted in the train and test dataset were fragmented into a single sentence, so CBOW modeling was also used to determine which was better model. Using CBOW averaged out the distribution in the result and basically trims some important aspects of the complex sentence data, and result in poorer representation of the sentences in news articles.
\end{quote}
\begin{quote}
Hierarchial softmax and negative sampling were also two concepts that were heavily dealt with in this paper. Softmax function generates a distribution to normalize multi dimensional vectors at the end of the neural network. This allows ranking of each word in the sentence of the news articles and create a distribution that best represents the sentence.
\end{quote}

\subsection{From Group to Individual Labels using Deep Features}
\label{sec:orge8b2323}
\begin{quote}
This paper provided information on how to use multiple instance learning to better predict the document label based on its sentence labels. The original intention was to implement Multiple Instance Learning using the algorithm presented by the authors of the paper. However, it turned out to be very difficult to do so, because it lacked the ability to run sentiment analysis in order to detect what is fake and what is not. Primary reason for this was there lacked enough data in both the fake and the real news to draw conclusion on what instances of sentences are fake or real.
\end{quote}
\begin{quote}
The main idea of the paper is as follows. The researchers came up with their classification method which is known as Group Instance Cost Function (GICF). It runs the same MIL algorithms to detect what instances of the bags of sentences are indeed a class or the other. Since the goal of running MIL is to predict whether a tiny portion of a document can overturn the majority of a document's sentiment to the opposite, it looks at the minor portion of the document in order to verify it.
\end{quote}

\section{Methods and Techniques}
\label{sec:org483c6f5}

\subsection{Natural Language Tool Kit}
\label{sec:orge755a34}
\begin{quote}
NLTK is a library for Python which provides multiple tools that help aid in natural language processing. One of the tools that were used in this from NLTK is Stemmer. A Stemmer can be of many different types. There are Porter Stemmer and Snowball Stemmer that stems the English words that are present in the documents and trims it to its most basic forms.
\end{quote}

\subsection{Gensim}
\label{sec:org303b2cb}
\begin{quote}
Gensim is a library that provided Summarize and Word2Vec. Summarize is a class that Gensim provides which analyzes a text and summarizes the text by using TextRank algorithm. TextRank algorithm is ran to extract the keywords of the text and put that into the generated summary at the end. And by using the generated summary reduced the run time because less words had to be evaluated. Gensim also provides Word2Vec module which was more heavily used towards the second half of the project. 
\end{quote}
\begin{quote}
Second approach included using Word2Vec. NLTK library for Python normalized the text data to create a better representation of sentences in news articles. Normalized text data was then ran with Gensim library to be turned into Word2Vec. The resulting vector represented the numerical values of each value per each neuron in the network created by the Gensim library. Gensim's implementation of Word2Vec has several parameters that includes type of estimation (hierarchial softmax or negative sampling), minimum count of words in a document to include in the computation, and the modeling type (skip-gram or CBOW).
\end{quote}

\subsection{Support Vector Machine}
\label{sec:orgcf4a997}
\begin{quote}
Very initial implementation of the project included Support Vector Machine. After summarization of the news article is done by NLTK and Gensim, the summarized news articles were converted into Tf-idf vectors to numerically represent each news article, with labels. Then the number of Tf-idf vectors were fed into the SVM classifier that scikit-learn library provided to create and train the best classifier for the given set of data. However, the whole idea was scrapped, because the accuracy score calculated for this implementation was heavily biased, and it sort of overfit the classifier into the project's specific selection of data.
\end{quote}

\subsection{K-Nearest Neighbor}
\label{sec:org0a6fc94}
\begin{quote}
One way to evaluate whether a document is fake or real was done by running pseudo K-Nearest Neighbor algorithm on the sentences of the documents. It counts the evaluation for every sentence in the news article whether it is evaluated to be fake or real. Then it counts the number of classes of the sentences in a given news article to determine the class of the news article as a whole.
\end{quote}

\section{Discussion and Results}
\label{sec:org578593f}

\subsection{Datasets}
\label{sec:orgc1ab102}
\begin{quote}
The dataset used for this project was Getting Real About Fake News and News Summary. The first dataset, Getting Real About Fake News, consisted of data about fake news. Attributes of fake news includes uuid, order in thread, author, published date, title, text of the news, language, crawled date, site url, country, domain rank, thread title, spam score, main img url, replies count, participants count, likes, comments, shares, and type. Despite the richness of the collected data from the dataset, only few attributes were used. The attributes that were used for this project was uuid, title, complete text, and language. Other attributes were disregarded because the next dataset, News Summary, only consisted basic attributes like title, author, and complete text.
\end{quote}

\begin{quote}
The second dataset used for the project was News Summary. This data set, unlike the prior one, only contains the basic attributes: title, summary, complete text, author, and date. The attributes used from this dataset include title, complete text, and summary.
\end{quote}

\begin{quote}
One possible problem with the use of two datasets was likely caused by having different number of data for each class. Approximately, 12403 fake news were used to model the Word2Vec, but only 4514 real news were used to model the Word2Vec. Because of this, more test data leaned towards being fake rather than being real, and showed a strange results where having less train data increased the overall accuracy. Because real news data only consisted of 30\% of the fake news data, and also because both of the datasets did not have a huge data in them, a good training model could not be developed. 
\end{quote}

\subsection{Evaluation Metrics}
\label{sec:org8684e90}
\begin{quote}
F-1 score was used to determine the scoring metric. This was chosen because of the binary classification of a news being either fake or real. Instead of the traditional positive/negative classification used for F-1 scoring, fake/real was used. Fake constitutes for the negative, and real constitutes for the positive.
\end{quote}

\subsection{Experimental Results}
\label{sec:org062e06c}
\begin{quote}
Using Skip-gram modeling for Word2Vec generally yielded poorer F1 score than the CBOW modeling. The major suspect of poor Skip-gram performance may be due to the fact that the original text gets normalized. When running the Word2Vec from the Gensim library, it takes in an argument that specifies the minimum occurrence of the words in order to use that for the calculation and generation of Word2Vec. However this removes a lot of words from the dataset as well as from the instances of sentences in each news articles and could result in a poor representation of the skip-gram model.
\end{quote}

\begin{quote}

\end{quote}

\begin{center}
Using count\(_{\text{min}}\) > 10, Hierarchial Softmax, Skip-gram, Using K-NN to determine class
\end{center}
\begin{center}
\begin{tabular}{llllrrrrr}
\hline
CV \% & Precision & Recall & Error & True Fake & False Fake & True Real & False Real & F1\\
\hline
90\% & 27.4\% & 76.4\% & 65.4\% & 208 & 113 & 366 & 970 & 0.40\\
80\% & 25.8\% & 78.5\% & 66.8\% & 404 & 196 & 715 & 2051 & 0.39\\
70\% & 24.9\% & 78.7\% & 69.3\% & 484 & 289 & 1066 & 3217 & 0.38\\
\hline
\end{tabular}
\end{center}

\begin{center}
Using count\(_{\text{min}}\) > 10, Hierarchial Softmax, Skip-gram
\end{center}
\begin{center}
\begin{tabular}{llllrrrrr}
\hline
CV \% & Precision & Recall & Error & True Fake & False Fake & True Real & False Real & F1\\
\hline
90\% & 25.1\% & 63.4\% & 74.8\% & 112 & 175 & 304 & 1066 & 0.33\\
80\% & 21.3\% & 66.9\% & 75.8\% & 207 & 302 & 609 & 2248 & 0.32\\
70\% & 21.0\% & 67.7\% & 76.8\% & 253 & 437 & 918 & 3448 & 0.32\\
\hline
\end{tabular}
\end{center}

\begin{center}
Using count\(_{\text{min}}\) > 10, Hierarchial Softmax, CBOW, Using K-NN to determine class
\begin{center}
\begin{tabular}{lll}
CV = 90\% & Predicted Fake & Predicted Real\\
\hline
Actual Fake & TF = 515 & FR = 663\\
Actual Real & FF = 68 & TR = 411\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{lll}
CV = 80\% & Predicted Fake & Predicted Real\\
\hline
Actual Fake & TF = 1040 & FR = 1415\\
Actual Real & FF = 154 & TR = 754\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{lll}
CV = 70\% & Predicted Fake & Predicted Real\\
\hline
Actual Fake & TF = 1450 & FR = 2251\\
Actual Real & FF = 257 & TR = 1098\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{llllrrrrr}
\hline
CV \% & Precision & Recall & Error & True Fake & False Fake & True Real & False Real & F1\\
\hline
90\% & 38.2\% & 85.8\% & 44.1\% & 515 & 68 & 411 & 663 & 0.53\\
80\% & 34.9\% & 83.1\% & 50.8\% & 1040 & 154 & 757 & 1415 & 0.49\\
70\% & 32.8\% & 81.0\% & 49.6\% & 1450 & 257 & 1098 & 2251 & 0.47\\
\hline
\end{tabular}
\end{center}
\end{center}

\begin{center}
Using count\(_{\text{min}}\) > 10, Hierarchial Softmax, CBOW
\begin{center}
\begin{tabular}{lll}
CV = 90\% & Predicted Fake & Predicted Real\\
\hline
Actual Fake & TF = 483 & FR = 695\\
Actual Real & FF = 95 & TR = 384\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{lll}
CV = 80\% & Predicted Fake & Predicted Real\\
\hline
Actual Fake & TF = 903 & FR = 1552\\
Actual Real & FF = 175 & TR = 736\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{lll}
CV = 70\% & Predicted Fake & Predicted Real\\
\hline
Actual Fake & TF = 1275 & FR = 2426\\
Actual Real & FF = 295 & TR = 1060\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{llllrrrrr}
\hline
CV \% & Precision & Recall & Error & True Fake & False Fake & True Real & False Real & F1\\
\hline
90\% & 35.6\% & 80.2\% & 47.7\% & 483 & 95 & 384 & 695 & 0.49\\
80\% & 32.2\% & 79.6\% & 50.8\% & 903 & 175 & 736 & 1552 & 0.46\\
70\% & 30.4\% & 78.2\% & 53.8\% & 1275 & 295 & 1060 & 2426 & 0.43\\
\hline
\end{tabular}
\end{center}
\end{center}

\section{Conclusions}
\label{sec:orgb3c14bf}

\begin{quote}
The main problem that contributed to the low score of the project, in my opinion, is due to the bad approach in data pre-processing. The approach that this project took was more based on the similarity between the fake news model and the real news model. Since there was a disparity between the fake news model and the real news model due to the amount of the data that were available, there were false claiming of a lot of fake news to be classified as real news which resulted in the failure of the model. If the majority of the data were the real news, just like in the real life, and only a few news were labeled fake in the dataset I used, I think it would have yielded a better result. The dilemma was that there could have been a way to weigh test dataset more heavily in fake news class, but that contradicts the examples from the real life and it would overfit in terms of the given datasets.
\end{quote}

\subsection{Directions for Future Work}
\label{sec:org0eb935e}

\begin{quote}
The topic was very interesting, and I have thought about the directions for the next steps. I think one of the more important part of the project should be on sentiment analysis. Currently, there is no real way to analyze which sentences are fake and which sentences seem real. I think it is the one sole obstacle in achieving a better score with the attempt of using MIL. If there was a way to intelligently analyze which sentences are more fake than the other, MIL can be implemneted on top of it. 
\end{quote}

\begin{quote}
Also another improvement can be done by collecting news data from variety of news sources by using web parsing libraries. There needs to be a sufficiently many number of data for the real news as well to create a model and train the best portrayal of the real news.
\end{quote}

\begin{quote}
Once data improvement and sentiment analysis is done, multiple instance learning can be implemented to analyze the news articles and check which literals constitutes to be fake news. I think it will definitely increase the score once all three steps (data gathering, sentiment analysis, multiple instance learning) are done, because it will have a better representation of the data and approach than how it is being approached now.
\end{quote}
\end{document}

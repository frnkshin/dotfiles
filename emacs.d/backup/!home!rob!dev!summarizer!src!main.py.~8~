# MAIN.PY
# Written by Hansol Shin on Nov-30-2017
from nltk.stem import SnowballStemmer
from gensim.summarization import summarize
from gensim.models import Word2Vec
from sklearn import svm
from sklearn.feature_extraction.text import TfidfVectorizer

from Dataset import FakeNews, RealNews
from datetime import datetime

import numpy as np
import re
import time
import nltk
import progressbar
import inquirer
import random

NUM_FAKES = 12999
NUM_REALS = 4515

def vectorize(dataset, unique_words):
    y = [0, 1]
    all_sentences = []
    all_clf = []
    for data in dataset:
        individual_vector = [0] * len(unique_words)
        try:
            text = dataset[data]['head']
            for words in text:
                for word in words:
                    individual_vector[unique_words.index(word)] += 1
        except KeyError:
            pass
        try:
            all_clf.append(dataset[data]['clf'])
        except KeyError:
            pass
        all_sentences.append(individual_vector)

    return all_sentences, all_clf

# Normalizes words to base words
def normalize(text):
    text = nltk.sent_tokenize(text)
    new_text = []
    stemmer = SnowballStemmer("english")
    for sentence in text:
        sentence = sentence.lower()
        sentence = re.sub("((\S+)?(http(s)?)(\S+))|((\S+)?(www)(\S+))|((\S+)?(\@)(\S+)?)", " ", sentence)
        sentence = re.sub("[^a-z ]", "", sentence)
        sentence = nltk.word_tokenize(sentence)
        sentence = [word for word in sentence if len(word) > 1]  # exclude 1 letter words
        sentence = [stemmer.stem(word) for word in sentence]
        new_text.append(sentence)

    return new_text

def rogue(model, gold):
    score = 0
    gold = gold.split(' ')
    model = model.split(' ')

    score = len(set(gold) & set(model)) / len(gold)
    return score

def normalize_set(dataset, num_count):
    with progressbar.ProgressBar(max_value=num_count) as progress:
        i = 0
        for data in dataset:
            dataset[data]['head'] = normalize(dataset[data]['head'])
            dataset[data]['text'] = normalize(dataset[data]['text'])
            try:
                dataset[data]['summary'] = normalize(dataset[data]['summary'])
            except KeyError:
                pass
            time.sleep(0.001)
            i += 1
            progress.update(i)
        return dataset

def summarize_set(dataset, num_count):
    with progressbar.ProgressBar(max_value=NUM_FAKES) as progress:
        scores = 0
        i = 0
        output = open("../res/fake_summaries.txt", 'w+')
        output.write('\n')

        for data in dataset:
            try:
                dataset[data]['summary'] = summarize(normalize(dataset[data]['text']), word_count=50)
            except ValueError:
                dataset[data]['summary'] = normalize(dataset[data]['text'])
            output.write(dataset[data]['summary'])
            scores += rogue(dataset[data]['text'], dataset[data]['summary'])
            time.sleep(0.001)
            i += 1
            progress.update(i)
        print(str(scores / NUM_FAKES))

def load_summaries(dataset, num_count, path):
    with open(path, 'r') as f:
        for data in dataset:
            dataset[data]['summary'] = f.readline()
    return dataset

def load_normalized(dataset, num_count, path):
    with open(path, 'r') as f:
        for data in dataset:
            dataset[data]['head'] = f.readline()
            dataset[data]['text'] = f.readline()
            dataset[data]['summary'] = f.readline()
    return dataset

# Main Function
if __name__ == "__main__":
    start = datetime.now()

    # LOAD DATASETS
    fake = FakeNews()
    real = RealNews()
    fake_news = fake.load_data()
    real_news = real.load_data()

    # NORMALIZATION
    confirm = {
        inquirer.Confirm('confirmed',
                         message="Do you want to normalize data?",
                         default=True),
    }
    confirmation = inquirer.prompt(confirm)
    if confirmation['confirmed']:
        # Normalize real news text
        real_news = normalize_set(real_news, NUM_REALS)
        fake_news = normalize_set(fake_news, NUM_FAKES)
    else:
        real.data = load_normalized(real_news, NUM_REALS, "../res/real_normalized.txt")
        fake.data = load_normalized(fake_news, NUM_FAKES, "../res/fake_normalized.txt")

    # SUMMARIZATION
    '''
    confirm = {
        inquirer.Confirm('confirmed',
                         message="Do you want to summarize data?",
                         default=True),
    }
    confirmation = inquirer.prompt(confirm)
    if confirmation['confirmed']:
        fake_news = summarize_set(fake_news, NUM_FAKES)
    else:
        fake_news = load_summaries(fake_news, NUM_FAKES, "../res/fake_summaries.txt")
    '''

    # DIVIDING DATA INTO TEST/TRAIN 0.8 to 0.2
    test = {}
    train = {}
    test_count = 0
    train_count = 0
    for i in range(0, NUM_FAKES):
        if random.random() > .8:
            train[train_count] = fake_news[i + 1]
            train[train_count]['clf'] = 1
            train_count += 1
        else:
            test[test_count] = fake_news[i + 1]
            test_count += 1

    '''
    for i in range(0, NUM_REALS):
        if random.random() > .8:
            train[train_count] = real_news[i + 1]
            train[train_count]['clf'] = 0
            train_count += 1
        else:
            test[test_count] = real_news[i + 1]
            test_count += 1
    '''

    for i in range(0, NUM_REALS):
        train[train_count] = real_news[i + 1]
        train[train_count]['clf'] = 0
        train_count += 1

    # unique words from train/test
    vocabs = {}
    for data in fake_news:
        try:
            text = fake_news[data]['head']
            for words in text:
                for word in words:
                    vocabs[word] = 1
        except KeyError:
            pass
    for data in real_news:
        try:
            text = real_news[data]['head']
            for words in text:
                for word in words:
                    vocabs[word] = 1
        except KeyError:
            pass
    unique_words = [vocab for vocab in vocabs.keys()]

    train_vect, train_clf = vectorize(train, unique_words)
    test_vect, test_clf = vectorize(test, unique_words)
    npa = np.asarray(test_vect, dtype=np.float32)
    clf = svm.SVC()
    clf.fit(train_vect, train_clf)

    ans_key = {}
    ans_key[0] = 0
    ans_key[1] = 0
    for i in range(0, len(train_vect)):
        print(i)
        ans = clf.predict([npa[i]])[0]
        ans_key[ans] += 1
        print(ans_key)
